{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "c9ad4a9e6e96b5b8cce921f386ec866a",
     "grade": false,
     "grade_id": "cell-d486a2d083662f95",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "# CL2 - Intro to Convolution Neural Networks in Keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this computer lab we'll show you an example of how to define, train, and assess the performance of convolutional neural networks using Keras. For this task, we will use the MNIST dataset, which comprises of 70.000 28x28 grayscale images of handwritten digits (60k for training, 10k for testing).\n",
    "\n",
    "Our goal is to build a convolutional neural network that takes as input the grayscale image of a handwritten digit, and outputs its corresponding label. As usual, we start by importing the required packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# The package for importing the dataset (already provided by Keras)\n",
    "from keras.datasets import mnist\n",
    "\n",
    "# Packages for defining the architecture of our model\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Flatten, BatchNormalization\n",
    "from keras.layers.convolutional import Conv2D, MaxPooling2D\n",
    "\n",
    "# One-hot encoding\n",
    "from keras.utils import np_utils\n",
    "\n",
    "# Callbacks for training\n",
    "from keras.callbacks import TensorBoard, EarlyStopping\n",
    "\n",
    "# Ploting\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# Ndarray computations\n",
    "import numpy as np\n",
    "\n",
    "# Confusion matrix for assessment step\n",
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Loading and visualizing the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can load the MNIST dataset using the function `load_data()`, which already splits it into training and test sets. If this is the first time you're running this cell, you will require internet access to download the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://s3.amazonaws.com/img-datasets/mnist.npz\n",
      "11493376/11490434 [==============================] - 3s 0us/step\n"
     ]
    }
   ],
   "source": [
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see the range of pixel values by running the following cell:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Min: 0 \n",
      "Max: 255\n"
     ]
    }
   ],
   "source": [
    "print('Min:', X_train.min(), '\\nMax:', X_train.max())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And also the shapes of the training set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60000, 28, 28)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60000,)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The shape of `X_train` can be interpreted as (samples, width, height). So we can see that the training set is comprised indeed of 60k images, each 28x28. Doing the same for the test set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 28, 28)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000,)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "shows us that we have 10k images in the test set, also 28x28."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can take a look at one of the examples in the training set by simply indexing `X_train` in its first dimension:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,  42, 118,\n",
       "        219, 166, 118, 118,   6,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0, 103, 242, 254,\n",
       "        254, 254, 254, 254,  66,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,  18, 232, 254,\n",
       "        254, 254, 254, 254, 238,  70,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0, 104, 244,\n",
       "        254, 224, 254, 254, 254, 141,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0, 207,\n",
       "        254, 210, 254, 254, 254,  34,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,  84,\n",
       "        206, 254, 254, 254, 254,  41,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         24, 209, 254, 254, 254, 171,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,  91,\n",
       "        137, 253, 254, 254, 254, 112,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,  40, 214, 250,\n",
       "        254, 254, 254, 254, 254,  34,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,  81, 247, 254,\n",
       "        254, 254, 254, 254, 254, 146,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0, 110, 246,\n",
       "        254, 254, 254, 254, 254, 171,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,  73,\n",
       "         89,  89,  93, 240, 254, 171,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   1, 128, 254, 219,  31,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   7, 254, 254, 214,  28,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0, 138, 254, 254, 116,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,  19, 177,  90,   0,   0,   0,   0,\n",
       "          0,  25, 240, 254, 254,  34,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0, 164, 254, 215,  63,  36,   0,  51,\n",
       "         89, 206, 254, 254, 139,   8,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,  57, 197, 254, 254, 222, 180, 241,\n",
       "        254, 254, 253, 213,  11,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0, 140, 105, 254, 254, 254, 254,\n",
       "        254, 254, 236,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   7, 117, 117, 165, 254,\n",
       "        254, 239,  50,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0]], dtype=uint8)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train[10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Which shows the 10th data point in the training set as a matrix of numbers. Since we know that each example is actually a grayscale image of a handwritten digit, is more conveninent to display it as an image instead:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAD8CAYAAAC4nHJkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMi4zLCBo\ndHRwOi8vbWF0cGxvdGxpYi5vcmcvIxREBQAADXNJREFUeJzt3X+oVHUax/HPs5VFaZiFdildXaut\nMNLtFkW1tInhLoYF/ZL+cNllr39UbCG4UZDCGtSSbitRYGgZlBWYm8SyGSFrwhJaSZlWmtzspujG\n7Yf1j6XP/nGPcbM73zN35pw5c+/zfoHMzHnmnPMw9bnnzJwfX3N3AYjnZ1U3AKAahB8IivADQRF+\nICjCDwRF+IGgCD8QFOEHgiL8QFDHt3JlZsbphEDJ3N3qeV9TW34zm2lmH5rZLjO7t5llAWgta/Tc\nfjM7TtJHkmZI6pG0WdIcd9+emIctP1CyVmz5L5O0y913u/shSc9Lmt3E8gC0UDPhP0vSp/1e92TT\nfsTMusxsi5ltaWJdAArWzA9+A+1a/GS33t2XS1ousdsPtJNmtvw9ksb3e322pL3NtQOgVZoJ/2ZJ\n55rZJDMbIek2SeuKaQtA2Rre7Xf3783sTkmvSjpO0kp3f7+wzgCUquFDfQ2tjO/8QOlacpIPgKGL\n8ANBEX4gKMIPBEX4gaAIPxAU4QeCIvxAUIQfCIrwA0ERfiAowg8ERfiBoAg/EBThB4Ii/EBQhB8I\nivADQRF+ICjCDwRF+IGgWjpEN8px4YUX1qzNmjUrOW9XV1eyvnnz5mT9nXfeSdZTHn300WT90KFD\nDS8b+djyA0ERfiAowg8ERfiBoAg/EBThB4Ii/EBQTY3Sa2bdkg5KOizpe3fvzHk/o/Q2YN68ecn6\nI488UrM2cuTIotspzLXXXpusb9iwoUWdDC/1jtJbxEk+v3H3zwtYDoAWYrcfCKrZ8Luk9Wb2lpml\nzxMF0Faa3e2/0t33mtlYSa+Z2QfuvrH/G7I/CvxhANpMU1t+d9+bPR6QtFbSZQO8Z7m7d+b9GAig\ntRoOv5mdYmajjj6XdJ2kbUU1BqBczez2j5O01syOLuc5d/93IV0BKF1Tx/kHvTKO8zdkzJgxyfqO\nHTtq1saOHVt0O4X58ssvk/Vbb701WV+/fn2R7Qwb9R7n51AfEBThB4Ii/EBQhB8IivADQRF+IChu\n3T0E9Pb2JusLFy6sWVuyZEly3pNPPjlZ37NnT7I+YcKEZD1l9OjRyfrMmTOTdQ71NYctPxAU4QeC\nIvxAUIQfCIrwA0ERfiAowg8ExSW9w9zWrVuT9YsvvjhZ37YtfX+WKVOmDLqnek2ePDlZ3717d2nr\nHsq4pBdAEuEHgiL8QFCEHwiK8ANBEX4gKMIPBMX1/MPc4sWLk/X7778/WZ86dWqR7QzKiBEjKlt3\nBGz5gaAIPxAU4QeCIvxAUIQfCIrwA0ERfiCo3Ov5zWylpFmSDrj7lGzaGEkvSJooqVvSLe7+Re7K\nuJ6/7Zx55pnJet698S+66KIi2/mRNWvWJOs33XRTaeseyoq8nv9pSceOnnCvpNfd/VxJr2evAQwh\nueF3942Sjh0yZrakVdnzVZJuKLgvACVr9Dv/OHffJ0nZ49jiWgLQCqWf229mXZK6yl4PgMFpdMu/\n38w6JCl7PFDrje6+3N073b2zwXUBKEGj4V8naW72fK6kl4tpB0Cr5IbfzFZL+q+kX5pZj5n9UdJD\nkmaY2U5JM7LXAIaQ3O/87j6nRml6wb2gBLfffnuynnff/jLvy59n06ZNla07As7wA4Ii/EBQhB8I\nivADQRF+ICjCDwTFEN1DwPnnn5+sr127tmbtnHPOSc57/PHte/d2huhuDEN0A0gi/EBQhB8IivAD\nQRF+ICjCDwRF+IGg2vcgL35wwQUXJOuTJk2qWWvn4/h57rnnnmT9rrvualEnwxNbfiAowg8ERfiB\noAg/EBThB4Ii/EBQhB8IaugeBA4kdb2+JC1YsKBm7eGHH07Oe9JJJzXUUyt0dHRU3cKwxpYfCIrw\nA0ERfiAowg8ERfiBoAg/EBThB4LKPc5vZislzZJ0wN2nZNMWSfqTpP9lb7vP3f9VVpNIW7ZsWc3a\nzp07k/OOHj26qXXn3S/gscceq1k79dRTm1o3mlPPlv9pSTMHmP53d5+a/SP4wBCTG3533yiptwW9\nAGihZr7z32lm75rZSjM7rbCOALREo+F/QtJkSVMl7ZO0pNYbzazLzLaY2ZYG1wWgBA2F3933u/th\ndz8i6UlJlyXeu9zdO929s9EmARSvofCbWf/LrW6UtK2YdgC0Sj2H+lZLukbSGWbWI2mhpGvMbKok\nl9QtaV6JPQIogbl761Zm1rqVoSXM0kPBL1q0qGbtgQceSM778ccfJ+vTp09P1j/55JNkfbhy9/R/\nlAxn+AFBEX4gKMIPBEX4gaAIPxAU4QeC4tbdaMqIESOS9bzDeSnfffddsn748OGGlw22/EBYhB8I\nivADQRF+ICjCDwRF+IGgCD8QFMf50ZTFixeXtuwVK1Yk6z09PaWtOwK2/EBQhB8IivADQRF+ICjC\nDwRF+IGgCD8QFLfurtPpp59es/bUU08l5129enVT9Sp1dHQk6x988EGy3sww3JMnT07Wd+/e3fCy\nhzNu3Q0gifADQRF+ICjCDwRF+IGgCD8QFOEHgsq9nt/Mxkt6RtKZko5IWu7u/zCzMZJekDRRUrek\nW9z9i/JardayZctq1q6//vrkvOedd16yvnfv3mT9s88+S9Z37dpVs3bJJZck583rbcGCBcl6M8fx\nlyxZkqznfS5oTj1b/u8lzXf3CyRdLukOM7tQ0r2SXnf3cyW9nr0GMETkht/d97n729nzg5J2SDpL\n0mxJq7K3rZJ0Q1lNAijeoL7zm9lESdMkvSlpnLvvk/r+QEgaW3RzAMpT9z38zGykpDWS7nb3r83q\nOn1YZtYlqaux9gCUpa4tv5mdoL7gP+vuL2WT95tZR1bvkHRgoHndfbm7d7p7ZxENAyhGbvitbxO/\nQtIOd1/ar7RO0tzs+VxJLxffHoCy5F7Sa2ZXSXpD0nvqO9QnSfep73v/i5ImSNoj6WZ3781Z1pC9\npPfyyy+vWVu6dGnNmiRdccUVTa27u7s7Wd++fXvN2tVXX52cd9SoUY209IO8/39Sl/xeeumlyXm/\n/fbbhnqKrt5LenO/87v7Jkm1FjZ9ME0BaB+c4QcERfiBoAg/EBThB4Ii/EBQhB8Iilt3FyDv0tTU\nJbeS9PjjjxfZTkv19iZP7Uje8hzl4NbdAJIIPxAU4QeCIvxAUIQfCIrwA0ERfiCoum/jhdrmz5+f\nrJ944onJ+siRI5ta/7Rp02rW5syZ09Syv/rqq2R9xowZTS0f1WHLDwRF+IGgCD8QFOEHgiL8QFCE\nHwiK8ANBcT0/MMxwPT+AJMIPBEX4gaAIPxAU4QeCIvxAUIQfCCo3/GY23sw2mNkOM3vfzP6cTV9k\nZp+Z2dbs3+/KbxdAUXJP8jGzDkkd7v62mY2S9JakGyTdIukbd3+k7pVxkg9QunpP8sm9k4+775O0\nL3t+0Mx2SDqrufYAVG1Q3/nNbKKkaZLezCbdaWbvmtlKMzutxjxdZrbFzLY01SmAQtV9br+ZjZT0\nH0kPuvtLZjZO0ueSXNJf1ffV4A85y2C3HyhZvbv9dYXfzE6Q9IqkV9196QD1iZJecfcpOcsh/EDJ\nCruwx8xM0gpJO/oHP/sh8KgbJW0bbJMAqlPPr/1XSXpD0nuSjmST75M0R9JU9e32d0ual/04mFoW\nW36gZIXu9heF8APl43p+AEmEHwiK8ANBEX4gKMIPBEX4gaAIPxAU4QeCIvxAUIQfCIrwA0ERfiAo\nwg8ERfiBoHJv4FmwzyV90u/1Gdm0dtSuvbVrXxK9NarI3n5e7xtbej3/T1ZutsXdOytrIKFde2vX\nviR6a1RVvbHbDwRF+IGgqg7/8orXn9KuvbVrXxK9NaqS3ir9zg+gOlVv+QFUpJLwm9lMM/vQzHaZ\n2b1V9FCLmXWb2XvZyMOVDjGWDYN2wMy29Zs2xsxeM7Od2eOAw6RV1FtbjNycGFm60s+u3Ua8bvlu\nv5kdJ+kjSTMk9UjaLGmOu29vaSM1mFm3pE53r/yYsJn9WtI3kp45OhqSmf1NUq+7P5T94TzN3f/S\nJr0t0iBHbi6pt1ojS/9eFX52RY54XYQqtvyXSdrl7rvd/ZCk5yXNrqCPtufuGyX1HjN5tqRV2fNV\n6vufp+Vq9NYW3H2fu7+dPT8o6ejI0pV+dom+KlFF+M+S9Gm/1z1qryG/XdJ6M3vLzLqqbmYA446O\njJQ9jq24n2PljtzcSseMLN02n10jI14XrYrwDzSaSDsdcrjS3X8l6beS7sh2b1GfJyRNVt8wbvsk\nLamymWxk6TWS7nb3r6vspb8B+qrkc6si/D2Sxvd7fbakvRX0MSB335s9HpC0Vn1fU9rJ/qODpGaP\nByru5wfuvt/dD7v7EUlPqsLPLhtZeo2kZ939pWxy5Z/dQH1V9blVEf7Nks41s0lmNkLSbZLWVdDH\nT5jZKdkPMTKzUyRdp/YbfXidpLnZ87mSXq6wlx9pl5Gba40srYo/u3Yb8bqSk3yyQxmPSjpO0kp3\nf7DlTQzAzH6hvq291HfF43NV9mZmqyVdo76rvvZLWijpn5JelDRB0h5JN7t7y394q9HbNRrkyM0l\n9VZrZOk3VeFnV+SI14X0wxl+QEyc4QcERfiBoAg/EBThB4Ii/EBQhB8IivADQRF+IKj/A+Rq/ARM\n9qglAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(X_train[10], cmap='gray');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And we can see the corresponding ground truth label by indexing `y_train` with the same index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train[10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we saw previously, the shape of the inputs is:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60000, 28, 28)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60000, 28, 28)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, Keras expects input images to have their dimensions in the following format `[samples][width][height][channels]`. Although we have grayscale images (i.e. with only one channel, instead of 3 like RGB images), we should reshape the input to conform to this by adding a new dimension."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_train.reshape(X_train.shape[0], 28, 28, 1)\n",
    "X_test = X_test.reshape(X_test.shape[0], 28, 28, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60000, 28, 28, 1)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And it's more convenient to work with `ndarray`s of floats, instead of integers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_train.astype('float32')\n",
    "X_test = X_test.astype('float32')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also want to normalize the pixel values to range from 0 to 1 (instead of 0 to 255),"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_train / 255\n",
    "X_test = X_test / 255"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And finally, one hot-encode the output labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = np_utils.to_categorical(y_train)\n",
    "y_test = np_utils.to_categorical(y_test)\n",
    "num_classes = y_test.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 0., 1., 0., 0., 0., 0., 0., 0.], dtype=float32)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train[10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that all the preprocessing steps were taken care of, we are ready to create a tentative model. The following code defines a model with the following layers, from input to output:\n",
    "\n",
    "- Convolutional layer, 30 filters of size 5x5.\n",
    "- ReLU\n",
    "- 2x2 MaxPooling layer\n",
    "- Fully connected layer with 128 neurons\n",
    "- ReLU\n",
    "- Fully connected layer with 10 neurons\n",
    "- Softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def base_model():\n",
    "    \n",
    "    # create model\n",
    "    model = Sequential()\n",
    "    model.add(Conv2D(30, (5, 5), input_shape=(28, 28, 1), activation='relu'))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(128, activation='relu'))\n",
    "    model.add(Dense(num_classes, activation='softmax'))\n",
    "    \n",
    "    # Compile model\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the convolutional layer was implemented using a [`Conv2D` layer](https://keras.io/layers/convolutional/#conv2d) and the max pooling layer was implemented using a [`MaxPooling2D` layer](https://keras.io/layers/pooling/#maxpooling2d). Take a look at Keras help page for these layers to obtain more information about them. \n",
    "\n",
    "Also, note that in order for Keras to connect the output of the MaxPooling layer (which has shape `(batch, width, heigth, channels)`) to the fully connected layer, it was necessary to first use a [`Flatten` layer](https://keras.io/layers/core/#flatten), which \"flattens\" its input, squashing all dimensions aside from the batch dimension together. For example, if the input has shape `(32, 28, 28, 3)`, the output will have shape `(32,2352)`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we defined the architecture, we can train it with the following cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/15\n",
      "60000/60000 [==============================] - 37s 612us/step - loss: 0.5401 - acc: 0.8545 - val_loss: 0.2190 - val_acc: 0.9371\n",
      "Epoch 2/15\n",
      "60000/60000 [==============================] - 35s 591us/step - loss: 0.1781 - acc: 0.9480 - val_loss: 0.1290 - val_acc: 0.9630\n",
      "Epoch 3/15\n",
      "60000/60000 [==============================] - 36s 605us/step - loss: 0.1109 - acc: 0.9689 - val_loss: 0.0874 - val_acc: 0.9751\n",
      "Epoch 4/15\n",
      "60000/60000 [==============================] - 38s 641us/step - loss: 0.0789 - acc: 0.9782 - val_loss: 0.0656 - val_acc: 0.9801\n",
      "Epoch 5/15\n",
      "60000/60000 [==============================] - 36s 604us/step - loss: 0.0626 - acc: 0.9823 - val_loss: 0.0550 - val_acc: 0.9834\n",
      "Epoch 6/15\n",
      "60000/60000 [==============================] - 36s 601us/step - loss: 0.0521 - acc: 0.9849 - val_loss: 0.0494 - val_acc: 0.9845\n",
      "Epoch 7/15\n",
      "60000/60000 [==============================] - 40s 663us/step - loss: 0.0448 - acc: 0.9873 - val_loss: 0.0469 - val_acc: 0.9847\n",
      "Epoch 8/15\n",
      "60000/60000 [==============================] - 37s 610us/step - loss: 0.0386 - acc: 0.9892 - val_loss: 0.0437 - val_acc: 0.9856\n",
      "Epoch 9/15\n",
      "60000/60000 [==============================] - 37s 616us/step - loss: 0.0353 - acc: 0.9901 - val_loss: 0.0430 - val_acc: 0.9850\n",
      "Epoch 10/15\n",
      "60000/60000 [==============================] - 37s 615us/step - loss: 0.0309 - acc: 0.9910 - val_loss: 0.0394 - val_acc: 0.9870\n",
      "Epoch 11/15\n",
      "60000/60000 [==============================] - 39s 655us/step - loss: 0.0289 - acc: 0.9918 - val_loss: 0.0403 - val_acc: 0.9866\n",
      "Epoch 12/15\n",
      "60000/60000 [==============================] - 36s 607us/step - loss: 0.0256 - acc: 0.9927 - val_loss: 0.0365 - val_acc: 0.9878\n",
      "Epoch 13/15\n",
      "60000/60000 [==============================] - 38s 627us/step - loss: 0.0228 - acc: 0.9939 - val_loss: 0.0415 - val_acc: 0.9870\n",
      "Epoch 14/15\n",
      "60000/60000 [==============================] - 35s 590us/step - loss: 0.0210 - acc: 0.9942 - val_loss: 0.0345 - val_acc: 0.9883\n",
      "Epoch 15/15\n",
      "60000/60000 [==============================] - 35s 587us/step - loss: 0.0184 - acc: 0.9950 - val_loss: 0.0358 - val_acc: 0.9873\n"
     ]
    }
   ],
   "source": [
    "# build the model\n",
    "model = base_model()\n",
    "\n",
    "# Fit the model\n",
    "tb = TensorBoard(log_dir='./logs/initial_setting')\n",
    "history = model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=15, batch_size=1024, callbacks=[tb])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try different architectures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def more_layers_model():\n",
    "    \n",
    "    model = Sequential()\n",
    "    model.add(Conv2D(30, (5, 5), input_shape=(28, 28, 1), activation='relu'))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    model.add(Conv2D(30, (5, 5), input_shape=(28, 28, 1), activation='relu'))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(128, activation='relu'))\n",
    "    model.add(Dense(num_classes, activation='softmax'))\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "def more_filters_model():\n",
    "    \n",
    "    model = Sequential()\n",
    "    model.add(Conv2D(100, (5, 5), input_shape=(28, 28, 1), activation='relu'))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(128, activation='relu'))\n",
    "    model.add(Dense(num_classes, activation='softmax'))\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "def more_neurons_model():\n",
    "    \n",
    "    model = Sequential()\n",
    "    model.add(Conv2D(30, (5, 5), input_shape=(28, 28, 1), activation='relu'))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(1024, activation='relu'))\n",
    "    model.add(Dense(num_classes, activation='softmax'))\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "def bnorm_model():\n",
    "    \n",
    "    model = Sequential()\n",
    "    model.add(Conv2D(30, (5, 5), input_shape=(28, 28, 1), activation='relu'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(128, activation='relu'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dense(num_classes, activation='softmax'))\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train all of them and compare the results using [TensorBoard](https://www.tensorflow.org/guide/summaries_and_tensorboard). To see the tensorboard results, open a terminal in the directory where this notebook is, and type `tensorboard --logdir=logs`. This will start the TensorBoard server and show you its address. Follow that address to access its web interface.\n",
    "\n",
    "Note: if you're running a notebook from the cloud, follow the instructions inside the `Instructions` folder about how to use TensorBoard instead. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model: more_layers_model\n"
     ]
    }
   ],
   "source": [
    "model_names = ['more_layers_model', 'more_filters_model', 'more_neurons_model', 'bnorm_model']\n",
    "\n",
    "for name in model_names:\n",
    "    print('Training model:',name)\n",
    "    model = globals()[name]()\n",
    "    tb = TensorBoard(log_dir='./logs/'+name)\n",
    "    model.fit(X_train,\n",
    "              y_train, \n",
    "              validation_data=(X_test, y_test),\n",
    "              epochs=15, \n",
    "              batch_size=1024, \n",
    "              callbacks=[tb],\n",
    "              verbose=0)\n",
    "    print('Done!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Choose the best one and train longer. Here we also use [early stopping](https://en.wikipedia.org/wiki/Early_stopping#Validation-based_early_stopping)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = base_model()\n",
    "\n",
    "# Set callbacks\n",
    "tb = TensorBoard(log_dir='./logs/final_model')\n",
    "estop = EarlyStopping(monitor='val_acc', patience=5)\n",
    "\n",
    "# Train the model\n",
    "model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=50, batch_size=1024, callbacks=[tb, estop])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Assessment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we found our best model, we can evaluate its performance on the test set. In our case, we would like to compute its accuracy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = model.evaluate(X_test, y_test, verbose=0)\n",
    "print(\"Test accuracy: %.2f%%\" % (scores[1]*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = model.predict(X_test)\n",
    "y_pred = np.argmax(temp, axis=1)\n",
    "y_true = np.argmax(y_test, axis=1)\n",
    "confusion_matrix(y_true, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What can you conclude from these? Which classes are the easiest to misclassify? Was that expected?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
